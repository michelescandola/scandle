---
title: Biased Estimators - Ridge and Lasso regressions
author: Michele Scandola
date: '2020-08-22'
slug: biased-estimators-ridge-and-lasso-regressions
categories:
- Statistics
tags:
- (Generalized) Linear Models
- R
subtitle: ''
lastmod: '2020-08-22T17:30:13+02:00'
authorLink: 'https://michelescandola.netlify.app/'
description: ''
hiddenFromHomePage: yes
hiddenFromSearch: no
featuredImage: ''
featuredImagePreview: ''
toc:
  enable: yes
math:
  enable: yes
lightgallery: no
license: ''
---


<div id="TOC">
true
</div>

<pre class="r"><code>library(knitr)
library(captioner)
fig_nums &lt;- captioner()
cod_nums &lt;- captioner( prefix = &quot;Code&quot; )</code></pre>
<p>Some days ago I wrote some notes concerning
<a href="/presentations/20200819linearregression/unbiased-linear-regression/index.html">unbiased estimators</a>,
that are the estimators that we commonly use with linear models, and statistical
analyses such as ANOVA, linear regression, etc…</p>
<p><em>Unbiased</em> estimators try to estimate the best unknown but not casual,
<span class="math inline">\(\beta\)</span> coefficients of the <em>independent variables</em> (i.v.)
to explain the observed <em>dependent variable</em> <span class="math inline">\(y\)</span> (d.v.),
minimising the error between the variables that can be sampled from the
linear model (<span class="math inline">\(\hat{y}\)</span>) and the observed data (<span class="math inline">\(y\)</span>).</p>
<p>In other terms, they try to reduce the error between <span class="math inline">\(y\)</span> and <span class="math inline">\(\hat{y}\)</span> with
different methodologies. We can also say that they look for the values
of the coefficients that better “explain the variance”.</p>
<div id="problems-with-the-unbiased-estimators" class="section level2">
<h2>Problems with the Unbiased Estimators</h2>
<p>That’s great! However, having models that predict so brilliantly the behaviour
of a specific sample of data coming from a statistical population, can lead
to the problem that the model is not actually so useful in predicting new data
and, therefore, it has not really captured the values of the coefficients
at a <em>population-level</em>. This is called <em>overfitting</em>.</p>
<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Overfitting"><em>Overfitting</em></a>:
the production of an analysis that corresponds too
closely or exactly to a particular set of data, and may therefore fail to fit
additional data or predict future observations reliably</p>
</blockquote>
<p>This may also be caused by the use of an excessive number of i.v. in the
analysis.</p>
<p>In order to prevent this problem, <em>biased</em> estimators have been proposed.</p>
<blockquote>
<p><em>Biased estimators</em>: algorithms to estimate the values of the coefficients
with penalisation coefficients, in order to have coefficients that do not
allow the perfect correspondence between <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(y\)</span>.</p>
</blockquote>
<p>Concepts that are often used to understand better when a statistical model
is a good model are the <em>bias</em> and the <em>variance</em>. Please, note that in this
context <em>bias</em> has
no relation with cognitive bias, and <em>variance</em> in this context is not the
dispersion statistics that we all know.</p>
<blockquote>
<p>The <em>bias</em> is the error of the model in fitting the observed data <span class="math inline">\(y\)</span>.
A large <em>bias</em> means that the model <em>underfits</em> the sample of data.</p>
</blockquote>
<pre class="r"><code>x = 1:10
y = x^3/50 + rnorm(10, sd = 1)

plot(x,y)
abline(lm(y~x), col=&quot;red&quot;)
points(x , x^3/50, type=&quot;l&quot;, col=&quot;green&quot;)
legend(&quot;top&quot;, col = c(&quot;red&quot; , &quot;green&quot;), lty = 1,
       legend = c(&quot;Underfit&quot; , &quot;Correct fit&quot;))</code></pre>
<p><img src="/presentations/20200903biasedestimators/2020-08-22-biased-estimators-ridge-and-lasso-regressions_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Figure 1: Graphical representation of an hypothetical distribution fitted by two models, one with correct fit (y ~ x^3), and one that underfits the data (y ~ x)</p>
<blockquote>
<p>The <em>variance</em> is the error of the model in predicting new data.
A large <em>variance</em> means that the model <em>overfits</em> the
original sample of data <span class="math inline">\(y\)</span>.</p>
</blockquote>
<pre class="r"><code>x = seq(from = 1, to = 5, by = 0.1)
y = x^3/100 + cos(x*10)^2

x1 = seq(from = 1, to = 5, by = 0.001)
y1 = x1^3/100 + cos(x1*10)^2

plot(x,y)
points(x1, y1, type=&quot;l&quot;,  col =&quot; red&quot;)
points(x, x^3/100 + mean(cos(x*10)^2), type = &quot;l&quot;, col = &quot;green&quot;)
legend(&quot;topleft&quot;, col = c(&quot;red&quot; , &quot;green&quot;), lty = 1,
       legend = c(&quot;Overfit&quot; , &quot;Correct fit&quot;))</code></pre>
<p><img src="/presentations/20200903biasedestimators/2020-08-22-biased-estimators-ridge-and-lasso-regressions_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Figure 2: Graphical representation of an hypothetical distribution fitted by two models, one with correct fit (y ~ x^3), and one that overfits the data (y ~ x^3 + cos(x * 10)^2)</p>
<p>(to be honest, the overfitted curve is obtained by using exactly
y ~ x^3 + cos(x * 10)^2, but for simplicity let’s pretend that the
cosine part is just random variation)</p>
<p>A good model should have low <em>bias</em> and low <em>variance</em>, finding the trade-off between them.</p>
<pre class="r"><code>x = 1:100
y = x^5 / 1e10

plot( x = 1:100 , y , type = &quot;l&quot; , col = &quot;red&quot;)
points( x = 100:1 , y = y , type = &quot;l&quot;, col = &quot;blue&quot;)
points( x = 1:100 , y = y + y[100:1] + 0.1 , type = &quot;l&quot; , col = &quot;green&quot;)
legend(&quot;top&quot;, col = c(&quot;red&quot; , &quot;blue&quot; , &quot;green&quot;), lty = 1, legend = c(&quot;Bias&quot; , &quot;Variance&quot; , &quot;Error trade-Off&quot;))</code></pre>
<p><img src="/presentations/20200903biasedestimators/2020-08-22-biased-estimators-ridge-and-lasso-regressions_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Figure 3: Graphical representation of the Bias and Variance errors of the model, and hypothetical trade-off curve.
Please note that the Trade-off curve has been displaced for a better visualization</p>
<p>The most famous among the <em>biased estimators</em> are the:</p>
<ul>
<li>Ridge regression (a.k.a. L2 regularization)</li>
<li>Lasso regression (a.k.a. L1 regularization)</li>
</ul>
</div>
<div id="ridge-regression" class="section level2">
<h2>Ridge regression</h2>
<p>Ridge regression tries to find the best coefficients (<span class="math inline">\(\beta\)</span>s) that describe
our observed sample of data (<span class="math inline">\(y\)</span>), penalising the computation of the
coefficients with a coefficient <span class="math inline">\(\lambda\)</span>.</p>
<p>Therefore, the idea is to find the <span class="math inline">\(\beta\)</span>s of the model
that best fit the data, with an additional
penalysing term <span class="math inline">\(\lambda\)</span>.</p>
<p><span class="math inline">\(\beta = \underset{\beta}{\operatorname{arg min}}~S(\beta, \lambda)\)</span></p>
<p>where</p>
<p><span class="math inline">\(S(\beta, \lambda) = \sum_{i=1}^{n} \frac{1}{2} ( y_i - \sum_{j=1}^p x_{i,j} \beta_{j})^2+\lambda \sum_{j=1}^p \beta_j^2\)</span></p>
<p><span class="math inline">\(\lambda\)</span> is the <em>complexity</em> parameter, and penalises the <span class="math inline">\(\beta\)</span>s, limiting
<em>overfitting</em> and shrinking large <span class="math inline">\(\beta\)</span>s.</p>
<p>In order to better understand how ridge regression works,
we can use an iterative, inefficient, and so simplified to be simply wrong,
function just to have an idea concerning how ridge regression works.</p>
<p>In this function we create some random data gaussianally distributed,
coming from a linear model with mean 1 (<span class="math inline">\(\beta_0\)</span>) and
regressor <span class="math inline">\(\beta_1 = 2\)</span>.</p>
<p>The same data used for <a href="/presentations/20200819linearregression/unbiased-linear-regression/index.html">unbiased estimators</a>.</p>
<p>We set our <span class="math inline">\(\lambda\)</span> to 0.1.</p>
<pre class="r"><code>## creating the data
set.seed( 1 )
# independent variable
x &lt;- 1:10
#   beta0 + i.v. * beta1 + epsilon
y &lt;- 1 + x * 2 + rnorm( n = 10 )

find_ridge_least_squares &lt;- function( y , x , beta0 , beta1 , lambda ) {
  
  # variable for the beta1 that best fits the data
  best_beta &lt;- c(NULL, NULL)
  # variable for memorizing the squared errors
  best_err  &lt;- Inf
  for(int in beta0){
    for( bet in beta1 ){
      yhat &lt;- int + x * bet
      err  &lt;- ( sum( ( y - yhat)^2 ) / 2 ) + ( lambda * ( int^2 + bet^2 ) )
      best_beta[1] &lt;- ifelse( best_err &gt; err,
                           int,
                           best_beta[1])
      best_beta[2] &lt;- ifelse( best_err &gt; err,
                           bet,
                           best_beta[2])
      best_err  &lt;- ifelse( best_err &gt; err,
                           err,
                           best_err)
    }
  }
  
  return( best_beta )
}

print(
  find_ridge_least_squares( y ,
                            x,
                            beta0 = seq(from = 0, to = 3, by = 0.01),
                            beta1 = seq(from = 1, to = 3, by = 0.01),
                            lambda = 0.01 )
)</code></pre>
<pre><code>## [1] 0.80 2.06</code></pre>
<p>Code 1: Exemplification code for a ridge regression</p>
<p>The estimations are not so far from the real values. However, let’s
see how the real function works, in library <code>glmnet</code>:</p>
<pre class="r"><code>library(&quot;glmnet&quot;)</code></pre>
<pre><code>## Carico il pacchetto richiesto: Matrix</code></pre>
<pre><code>## Loaded glmnet 4.0-2</code></pre>
<pre class="r"><code>fit &lt;- glmnet(y = y, x = cbind(1,x),
              lambda = 0.01, alpha = 0)

print( coef( fit ) )</code></pre>
<pre><code>## 3 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                    s0
## (Intercept) 0.8501507
##             .        
## x           2.0512822</code></pre>
<p>Code 2: Use of glmnet for ridge regression</p>
<p>The minimisation problem has a unique solution:</p>
<p><span class="math inline">\(\beta = (X^TX + \lambda I)^{-1}X^Ty\)</span></p>
<pre class="r"><code>X = cbind( 1 , x )

solve( t(X) %*% X + 0.01 * diag(2) ) %*% t(X) %*% y</code></pre>
<pre><code>##        [,1]
##   0.8286793
## x 2.0550354</code></pre>
<p>Code 3: Matrix computation for Ridge regression</p>
<p>In this case we can see that the algorithms used in the <code>glmnet</code> function
and in the matrix computation are different.</p>
<p><code>glmnet</code> is a complex function that allows to compute Ridge regression (alpha = 0),
Lasso regression (alpha = 1) and Elasticnet regression (alpha <span class="math inline">\(\in\)</span> [0, 1]),
and probably it is using
algorithms that are more advanced than the classical ones that I am using here.</p>
</div>
<div id="lasso-regression" class="section level2">
<h2>Lasso regression</h2>
<p>The Least Absolute Shrinkage and Selection Operator (Lasso) regression
is very similar to the Ridge regression.</p>
<p>In fact, it uses the <span class="math inline">\(\lambda\)</span> coefficient of penalisation already seen
in the Ridge regression.</p>
<p>Therefore, the minimisation problem is always:</p>
<p><span class="math inline">\(\beta = \underset{\beta}{\operatorname{arg min}}~S(\beta, \lambda)\)</span></p>
<p>but the <span class="math inline">\(S\)</span> function changes:</p>
<p><span class="math inline">\(S(\beta, \lambda) = \frac{1}{n}\sum_{i=1}^{n} ( y_i - \sum_{j=1}^p x_{i,j} \beta_{j})^2+\frac{\lambda}{2} \sum_{j=1}^p |\beta_j|\)</span></p>
<pre class="r"><code>find_lasso_least_squares &lt;- function( y , x , beta0 , beta1 , lambda ) {
  
  # variable for the beta1 that best fits the data
  best_beta &lt;- c(NULL, NULL)
  # variable for memorizing the squared errors
  best_err  &lt;- Inf
  for(int in beta0){
    for( bet in beta1 ){
      yhat &lt;- int + x * bet
      err  &lt;- ( sum( ( y - yhat)^2 ) / ( length(yhat) ) ) +
        ( lambda / 2 * abs( int + bet ) )
      best_beta[1] &lt;- ifelse( best_err &gt; err,
                           int,
                           best_beta[1])
      best_beta[2] &lt;- ifelse( best_err &gt; err,
                           bet,
                           best_beta[2])
      best_err  &lt;- ifelse( best_err &gt; err,
                           err,
                           best_err)
    }
  }
  
  return( best_beta )
}

print(
  find_lasso_least_squares( y ,
                            x,
                            beta0 = seq(from = 0, to = 3, by = 0.01),
                            beta1 = seq(from = 1, to = 3, by = 0.01),
                            lambda = 0.01 )
)</code></pre>
<pre><code>## [1] 0.80 2.06</code></pre>
<p>Code 4: Exemplification code for a Lasso regression</p>
<pre class="r"><code>fit &lt;- glmnet(y = y, x = cbind(1,x),
              lambda = 0.01)

print( coef( fit ) )</code></pre>
<pre><code>## 3 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                   s0
## (Intercept) 0.850325
##             .       
## x           2.051251</code></pre>
<p>Code 5: Use of glmnet for Lasso regression</p>
<p>The minimisation problem can be divided in two steps:</p>
<ol style="list-style-type: decimal">
<li>finding the <span class="math inline">\(\beta\)</span>s with OLS (<span class="math inline">\(\beta = (X^TX)^{-1}X^Ty\)</span>)</li>
<li>applying the Lasso penalisation (<span class="math inline">\((1+N \frac{\lambda}{2})^{-1}\)</span>)</li>
</ol>
<p><span class="math inline">\(\beta = (1+N \frac{\lambda}{2})^{-1}[(X^TX)^{-1}X^Ty]\)</span></p>
<pre class="r"><code>X = cbind( 1 , x )

(1 + nrow(X) * 0.01 / 2)^(-1) *
solve( t(X) %*% X ) %*% t(X) %*% y</code></pre>
<pre><code>##        [,1]
##   0.7915966
## x 1.9568877</code></pre>
<p>Also in this case the results are different from the <code>glmnet</code> function.
I suspect that in the <code>glmnet</code> function they are using more advanced versions
of the Lasso regression.</p>
</div>
